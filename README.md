# üèóÔ∏è Retail Lakehouse Project  
**Databricks ‚Ä¢ Apache Airflow ‚Ä¢ Delta Lake ‚Ä¢ Power BI**

---

## üìå Project Overview

This project implements a **production-grade Retail Data Lakehouse** using **Databricks**, **Apache Airflow**, **Delta Lake**, and **Power BI**, following a **Bronze ‚Üí Silver ‚Üí Gold** architecture with an automated **Data Quality (DQ) Gate**.

The solution demonstrates how raw transactional data can be ingested, validated, orchestrated, audited, and transformed into **trusted, analytics-ready datasets**, closely mirroring real-world enterprise data platforms.

---

## üéØ Key Objectives

- Build a scalable **Lakehouse architecture**
- Separate raw, clean, and business-ready data layers
- Enforce **data quality checks before analytics**
- Orchestrate pipelines using **Apache Airflow**
- Enable **safe reruns, observability, and failure handling**
- Deliver **Power BI‚Äìready Gold datasets**

---

## üß± Technology Stack

- **Databricks** ‚Äì Data processing & Delta Lake storage  
- **Apache Airflow (Dockerized)** ‚Äì External orchestration  
- **Delta Lake** ‚Äì ACID-compliant storage layers  
- **Power BI** ‚Äì Analytics and visualization  
- **Python / PySpark** ‚Äì Transformations and data quality logic  

---

## üèõÔ∏è High-Level Architecture

```text
Synthetic Retail Data
(Generated Transactions)
        ‚Üì
Bronze Layer
Raw Delta Tables
(Immutable Ingestion)
        ‚Üì
Silver Layer
Cleaned & Validated Data
(Deduplication, Parsing)
        ‚Üì
Data Quality Gate
Validation Rules
(Fail / Pass Thresholds)
        ‚Üì
Gold Layer
Business Aggregations
(BI-Optimized Tables)
        ‚Üì
Power BI Dashboards
Reporting & Insights
```

---

## üìÇ Repository Structure (Overview)

```text
retail-lakehouse/
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/               ‚Üí Airflow DAGs
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile          ‚Üí Airflow image
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml  ‚Üí Local orchestration
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/             ‚Üí Raw ingestion
‚îÇ   ‚îú‚îÄ‚îÄ silver/             ‚Üí Cleaning & DQ
‚îÇ   ‚îî‚îÄ‚îÄ gold/               ‚Üí Aggregations
‚îú‚îÄ‚îÄ data_generator/         ‚Üí Synthetic data scripts
‚îú‚îÄ‚îÄ data/                   ‚Üí Sample/reference data
‚îú‚îÄ‚îÄ docs/                   ‚Üí Architecture & DQ docs
‚îú‚îÄ‚îÄ powerbi/                ‚Üí Local dashboards
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore
```

---

## üöÄ Key Features

Bronze‚ÄìSilver‚ÄìGold Lakehouse architecture

External orchestration using Apache Airflow

Explicit Data Quality Gate before Gold layer

Idempotent, rerunnable pipelines

Power BI‚Äìready analytics datasets

---

## üìä Analytics Layer

The Gold layer outputs are designed to be directly consumed by **Power BI**, enabling:
- Revenue trends
- Store and product performance
- Customer-level insights
- Channel analysis

  ---

## üìä Power BI Dashboards

The Gold-layer datasets produced by the lakehouse are consumed directly by **Power BI** to enable business reporting and analytics.

### Dashboard Highlights

#### Overview Dashboard
![Overview Dashboard](powerbi/screenshots/Executive_Overview.png)

#### Revenue by Store
![Revenue by Store](powerbi/screenshots/Product_Performance.png)

#### Product Performance
![Product Performance](powerbi/screenshots/Store_Performance.png)

> Power BI `.pbix` files are intentionally excluded from version control.  
> Screenshots are provided for demonstration purposes.

---

## üß™ How to Run Locally (Airflow Orchestration)

This project runs **Apache Airflow locally via Docker Compose** to orchestrate Databricks notebooks (**Bronze ‚Üí Silver ‚Üí DQ Gate ‚Üí Gold**).

---

### ‚úÖ Prerequisites

- **Docker Desktop** installed and running
- A **Databricks workspace** + an existing cluster (or job cluster setup in the DAG)
- A **Databricks Personal Access Token (PAT)**
- Databricks notebooks available in Workspace (paths configured in the DAG)

---

### 1Ô∏è‚É£ Clone the repository

git clone https://github.com/‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà/retail-lakehouse.git
cd retail-lakehouse

### 2Ô∏è‚É£ Configure environment variables (local only)
Create/update this file:
airflow/.env

Add your Databricks details:
#### Databricks workspace host (NO https://)
DATABRICKS_HOST=adb-‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà.‚ñà‚ñà.azuredatabricks.net

#### Databricks Personal Access Token
DATABRICKS_TOKEN=‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

#### Cluster ID (if your DAG uses an existing cluster)
DATABRICKS_CLUSTER_ID=‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
üîí Security note: .env is intentionally excluded from version control via .gitignore.

### 3Ô∏è‚É£ Start Airflow (Docker Compose)
From the repo root:
cd airflow
docker compose up -d
Airflow UI will be available at:

http://localhost:8080

### 4Ô∏è‚É£ Login to Airflow
Use the credentials defined by your Airflow Docker setup.
(Common default for the official Airflow docker-compose is:)

Username: airflow

Password: airflow

(If your setup differs, check airflow/docker-compose.yaml.)

### 5Ô∏è‚É£ Trigger the DAG
In Airflow UI:

Go to DAGs

Enable: retail_lakehouse_bronze_silver_gold

Click ‚ñ∂ Trigger DAG

Monitor execution in Grid / Graph view and per-task logs.

### 6Ô∏è‚É£ Stop Airflow (when finished)

docker compose down
Optional: remove volumes for a clean reset:

docker compose down -v
üß≠ Databricks Notebook Paths (Configured in DAG)
Update the DAG or variables to match your Databricks Workspace paths:

Bronze: /Workspace/Users/‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà/retail-lakehouse/02_bronze_ingestion

Silver: /Workspace/Users/‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà/retail-lakehouse/03_silver_cleaning

Gold: /Workspace/Users/‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà/retail-lakehouse/04_gold_aggregations

‚úÖ Expected Outcome
After a successful run:

Bronze Delta tables are created/refreshed

Silver clean + rejected tables are produced

DQ gate validates data health (if enabled)

Gold aggregations are refreshed and ready for Power BI

---

## üß† Why This Project Matters

This project goes beyond simple ETL by demonstrating:
- Real-world **data governance practices**
- External orchestration instead of notebook chaining
- Production-style **quality enforcement**
- End-to-end ownership from ingestion to BI

---

## üìå Notes

- Secrets and environment variables are excluded via `.gitignore`
- Airflow logs and Power BI binaries are kept local only
- Detailed architecture and DQ logic can be found in `/docs`

---

## üë§ Author

**Kamran Habib**  
Data Analytics & Data Engineering Projects 
